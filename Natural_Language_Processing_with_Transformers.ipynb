{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "635ee7c6-3fc5-4094-9488-81d9036f1f41",
   "metadata": {},
   "source": [
    "## Natural Language Processing with Transformers (book)\n",
    "\n",
    "### https://www.oreilly.com/library/view/natural-language-processing/9781098136789/\n",
    "\n",
    "### https://campus.datacamp.com/courses/deep-learning-for-text-with-pytorch/advanced-topics-in-deep-learning-for-text-with-pytorch?ex=4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fab75e-73c9-4369-aae6-88ef15e9b9fc",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fd120c-b337-4ceb-9b82-aa1be10341b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'reading', 'a', 'book', 'now', '.', 'i', 'love', 'to', 'read', 'books', '!']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(\"I am reading a book now. I love to read books!\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dbac82-fc50-4577-904b-081e592f30e3",
   "metadata": {},
   "source": [
    "### Stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3203fdb1-bdf8-49c7-8fd3-4f366437ea68",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      2\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15bf550a-607b-492b-9f15-bb791cc211e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglisch\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      4\u001b[0m filtered_tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(filtered_tokens)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stop_words = set(stopwords.words('englisch'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e125bd4-5567-43a8-8767-38570438c414",
   "metadata": {},
   "source": [
    "### Sentiment analysis cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc76525f-42af-4d1e-b187-0ce955a5b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "class SentimentAnalysisCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv = nn.Conv1d(embedding_dim, embedding_dim, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(embedding_dim, 2)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text).permute(0, 2, 1)\n",
    "        conved = self.relu(self.conv(embedded))\n",
    "        conved = conved.mean(dim=2)\n",
    "        return self.linear(conved)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe76c420-2401-411e-9320-2abd0eb73d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'this', 'book', '.', 'i', 'do', 'not', 'like']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mscho\\anaconda3\\envs\\custom_transformers\\lib\\site-packages\\torchtext\\data\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(\"I love this book. I do not like\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "870c979d-fa92-43b0-9965-786b53153200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([86, 99, 96, 49, 62, 78, 39, 93, 101, 65, 88], 1)\n",
      "([72, 2, 65, 69, 11, 62, 65, 33, 3], 0)\n",
      "([18, 38, 57, 15, 12, 4], 1)\n",
      "([86, 74, 96, 67, 45, 62, 58, 50], 0)\n",
      "([34, 92, 28, 64, 97, 25, 19], 1)\n",
      "([86, 21, 96, 91, 62, 36], 0)\n",
      "([18, 1, 82, 64, 35, 62, 95], 1)\n",
      "([86, 33, 90, 80, 26, 81], 0)\n",
      "([31, 83, 62, 8, 27], 1)\n",
      "([86, 14, 96, 97, 46, 64, 10, 62, 73], 0)\n",
      "([72, 7, 65, 24, 62, 65, 51], 1)\n",
      "([86, 89, 96, 16, 62, 65, 63, 96, 40], 0)\n",
      "([34, 43, 99, 77, 41, 23, 37, 22, 87], 1)\n",
      "([86, 32, 96, 71, 62, 65, 68, 96, 29], 0)\n",
      "([34, 48, 20, 64, 65, 70], 1)\n",
      "([86, 66, 79, 44, 62, 65, 69, 80, 94], 0)\n",
      "([18, 17, 84, 15, 100, 42], 1)\n",
      "([86, 33, 96, 59, 62, 5, 37, 60], 0)\n",
      "([34, 61, 85, 55, 65, 9, 54], 1)\n",
      "([86, 76, 47, 80, 75, 62, 13], 0)\n",
      "([72, 52, 30, 74, 26, 6], 1)\n",
      "([72, 0, 56, 30, 74, 53, 98], 0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Sample sentences and labels\n",
    "book_samples = [\n",
    "    (\"The story was captivating and kept me hooked until the end.\".split(), 1),\n",
    "    (\"I found the characters shallow and the plot predictable\".split(), 0),\n",
    "    (\"An absolute masterpiece with stunning visuals.\".split(), 1),\n",
    "    (\"The movie was too slow and quite boring.\".split(), 0),\n",
    "    (\"A beautiful portrayal of a complex character.\".split(), 1),\n",
    "    (\"The dialogue was unrealistic and forced.\".split(), 0),\n",
    "    (\"An inspiring tale of hope and perseverance.\".split(), 1),\n",
    "    (\"The plot twists were very predictable.\".split(), 0),\n",
    "    (\"Excellent direction and outstanding performances.\".split(), 1),\n",
    "    (\"The film was a waste of time and money.\".split(), 0),\n",
    "    (\"I loved the cinematography and the soundtrack.\".split(), 1),\n",
    "    (\"The acting was subpar and the script was weak.\".split(), 0),\n",
    "    (\"A heartwarming story that brought tears to my eyes.\".split(), 1),\n",
    "    (\"The pacing was off and the ending was disappointing.\".split(), 0),\n",
    "    (\"A brilliant adaptation of the novel.\".split(), 1),\n",
    "    (\"The humor fell flat and the characters were annoying.\".split(), 0),\n",
    "    (\"An epic journey with breathtaking scenery.\".split(), 1),\n",
    "    (\"The plot was convoluted and hard to follow.\".split(), 0),\n",
    "    (\"A moving performance by the lead actor.\".split(), 1),\n",
    "    (\"The special effects were overdone and distracting.\".split(), 0),\n",
    "    (\"I love this movie very much.\".split(), 1),\n",
    "    (\"I did not this movie like it.\".split(), 0)\n",
    "]\n",
    "\n",
    "# Create vocabulary and word-to-index mapping\n",
    "tokens = set()\n",
    "for sentence, _ in book_samples:\n",
    "    tokens.update(sentence)\n",
    "tokens = list(tokens)\n",
    "\n",
    "word_to_idx = {word: i for i, word in enumerate(tokens)}\n",
    "vocab_size = len(tokens)\n",
    "embedding_dim = 10\n",
    "\n",
    "# Convert sentences to indices and create tensors\n",
    "data = [\n",
    "    ([word_to_idx.get(w, 0) for w in sentence], label)\n",
    "    for sentence, label in book_samples\n",
    "]\n",
    "\n",
    "# Print the generated data\n",
    "for sample in data:\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c06ac28-b9dc-4b42-86dc-2516032b421e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 0.7575240731239319\n",
      "epoch: 1, train loss: 0.7340717911720276\n",
      "epoch: 2, train loss: 0.7250593900680542\n",
      "epoch: 3, train loss: 0.722136914730072\n",
      "epoch: 4, train loss: 0.7211416959762573\n",
      "epoch: 5, train loss: 0.7207779884338379\n",
      "epoch: 6, train loss: 0.7206364870071411\n",
      "epoch: 7, train loss: 0.7204015254974365\n",
      "epoch: 8, train loss: 0.7202353477478027\n",
      "epoch: 9, train loss: 0.7202185988426208\n",
      "epoch: 10, train loss: 0.7202701568603516\n",
      "epoch: 11, train loss: 0.7201457023620605\n",
      "epoch: 12, train loss: 0.7202916741371155\n",
      "epoch: 13, train loss: 0.719943106174469\n",
      "epoch: 14, train loss: 0.7199276089668274\n",
      "epoch: 15, train loss: 0.7202886939048767\n",
      "epoch: 16, train loss: 0.7204899787902832\n",
      "epoch: 17, train loss: 0.7199614644050598\n",
      "epoch: 18, train loss: 0.7204902768135071\n",
      "epoch: 19, train loss: 0.7207635045051575\n",
      "epoch: 20, train loss: 0.7199158668518066\n",
      "epoch: 21, train loss: 0.7199355363845825\n",
      "epoch: 22, train loss: 0.7207097411155701\n",
      "epoch: 23, train loss: 0.721081554889679\n",
      "epoch: 24, train loss: 0.7202740907669067\n",
      "epoch: 25, train loss: 0.7210951447486877\n",
      "epoch: 26, train loss: 0.7215032577514648\n",
      "epoch: 27, train loss: 0.7201858758926392\n",
      "epoch: 28, train loss: 0.7202154994010925\n",
      "epoch: 29, train loss: 0.7213873863220215\n"
     ]
    }
   ],
   "source": [
    "import torch.optim \n",
    "\n",
    "sentimentanalysis_model = SentimentAnalysisCNN(vocab_size, embedding_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(sentimentanalysis_model.parameters(), lr=0.1)\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for sentence, label in data:\n",
    "        sentimentanalysis_model.zero_grad()\n",
    "        sentence = torch.LongTensor([word_to_idx.get(w, 0) for w in sentence]).unsqueeze(0)\n",
    "        outputs = sentimentanalysis_model(sentence)\n",
    "        label = torch.LongTensor([int(label)])\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"epoch: {epoch}, train loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca71fbc1-b1d9-4ef8-a1ce-116d6e625511",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {word: i for i, word in enumerate(tokens)}\n",
    "vocab_size = len(tokens)\n",
    "embedding_dim = 10\n",
    "book_samples = [\n",
    "    (\"The story was captivating and kept me hooked until the end.\".split(), 1),\n",
    "    (\"I found the characters shallow and the plot predictable\".split(), 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c90eee4-64fc-4d7e-bbf2-5787ee6d4b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book Review: I love this movie\n",
      "Sentiment: Negative\n",
      "\n",
      "Book Review: I do not like this movie\n",
      "Sentiment: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book_reviews = [\n",
    "    \"I love this movie\".split(),\n",
    "    \"I do not like this movie\".split()\n",
    "]\n",
    "for review in book_reviews:\n",
    "    # Convert the review words into tensor form\n",
    "    input_tensor = torch.LongTensor([word_to_idx.get(w, 0) for w in sentence]).unsqueeze(0)\n",
    "    #input_tensor = torch.LongTensor([word_to_idx[w] for w in review], dtype=torch.long).unsqueeze(0) \n",
    "    # Get the model's output\n",
    "    outputs = sentimentanalysis_model(input_tensor)\n",
    "    # Find the index of the most likely sentiment category\n",
    "    _, predicted_label = torch.max(outputs.data, 1)\n",
    "    # Convert the predicted label into a sentiment string\n",
    "    sentiment = \"Positive\" if predicted_label.item() == 1 else \"Negative\"\n",
    "    print(f\"Book Review: {' '.join(review)}\")\n",
    "    print(f\"Sentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86eac203-5e04-4536-b981-a8023cb77128",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\Users\\mscho\\anaconda3\\envs\\custom_transformers\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0984e96edf85461a9db409bb32fece14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mscho\\anaconda3\\envs\\custom_transformers\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mscho\\.cache\\huggingface\\hub\\models--nlptown--bert-base-multilingual-uncased-sentiment. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dcffb7f13824d58ad89c76a71d803c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/669M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526f84168e8646b7834cf148c4842f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480a63170bac483d9c9760442773d0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f1965a73944181a1f1f887ca5e7f15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>0.882212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label     score\n",
       "0  5 stars  0.882212"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "sentiment = sentiment_pipeline(\"I love this movie\")\n",
    "print(f\"Sentiment: {sentiment[0]['label']}\")\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "outputs = classifier(\"I love this movie\")\n",
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08be6183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a71071ef2145189829eb66730cdde8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/829 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mscho\\anaconda3\\envs\\custom_transformers\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mscho\\.cache\\huggingface\\hub\\models--dslim--bert-base-NER. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a3d304429d4fcfa685454dc2258318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622123bd8d774ef9b7df84bc0ef0c114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0c4516a6e941d8b811595f7b1ea538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7eca6de514b4e1d95e7f643100c2ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15355b906b7f4ef5ab1ec625b5fd32d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_entity_recognition = pipeline(\"ner\", model=\"dslim/bert-base-NER\")\n",
    "outputs = name_entity_recognition(\"I love this movie\")\n",
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a5d49d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\Users\\mscho\\anaconda3\\envs\\custom_transformers\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.638591468334198, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}\n"
     ]
    }
   ],
   "source": [
    "reader = pipeline(\"question-answering\")\n",
    "question = \"Where do I work?\"\n",
    "context = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "outputs = reader(question=question, context=context)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd4df500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derzeit sind 158297 Datensätze\n",
      "Die ersten 10 sind: ['amirveyseh/acronym_identification', 'ade-benchmark-corpus/ade_corpus_v2', 'UCLNLP/adversarial_qa', 'Yale-LILY/aeslc', 'nwu-ctext/afrikaans_ner_corpus', 'fancyzhx/ag_news', 'allenai/ai2_arc', 'google/air_dialogue', 'komari6/ajgt_twitter_ar', 'legacy-datasets/allegro_reviews']\n"
     ]
    }
   ],
   "source": [
    "from datasets import list_datasets\n",
    "#print(list_datasets('squad', split='train')[0])\n",
    "all_datasets = list_datasets()\n",
    "print(f\"Derzeit sind {len(all_datasets)} Datensätze\")\n",
    "print(f\"Die ersten 10 sind: {all_datasets[:10]}\")\n",
    "\n",
    "#from datasets import list_datasets\n",
    "#datasets_list = datasets.list_datasets()\n",
    "#print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2df0aa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mscho\\anaconda3\\envs\\custom_transformers\\lib\\site-packages\\datasets\\load.py:1491: FutureWarning: The repository for emotion contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/emotion\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667b70cd1f9240129b4212c050178bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ceffddb3304847a437960df52f79a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/3.28k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b52249a8834ba18d597b44b054c5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/8.78k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c9007980bc4deebf29ddfb2e38fccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/592k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a4c41f097b64af58a0874b2139f8faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/74.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae314bba971468e912749dc68083398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/74.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1d87a5590f49ca8fd7a7364a35aacf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d14312489854011b25112a9ba72dde3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a35ae0c38754cc795bc985d561ddcde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 16000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "emotions = load_dataset(\"emotion\")\n",
    "print(emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54d8c6b4-079d-4ffd-8247-b2b1ecdd2d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 16000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = emotions['train']\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8466bc32-47e3-4afc-b3a7-0d2b71dd2230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['i didnt feel humiliated', 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake', 'im grabbing a minute to post i feel greedy wrong', 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property', 'i am feeling grouchy'], 'label': [0, 0, 3, 2, 3]}\n",
      "['text', 'label']\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[:5])\n",
    "print(train_ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f29dc5c8-481f-403a-9a98-035e5afcfdc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ive been feeling a little burdened lately wasn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ive been taking or milligrams or times recomme...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i feel as confused about life as a teenager or...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i have been with petronas for years i feel tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i feel romantic too</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                            i didnt feel humiliated      0\n",
       "1  i can go from feeling so hopeless to so damned...      0\n",
       "2   im grabbing a minute to post i feel greedy wrong      3\n",
       "3  i am ever feeling nostalgic about the fireplac...      2\n",
       "4                               i am feeling grouchy      3\n",
       "5  ive been feeling a little burdened lately wasn...      0\n",
       "6  ive been taking or milligrams or times recomme...      5\n",
       "7  i feel as confused about life as a teenager or...      4\n",
       "8  i have been with petronas for years i feel tha...      1\n",
       "9                                i feel romantic too      2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "emotions.set_format(type=\"pandas\")\n",
    "df = emotions[\"train\"][:]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7b8ad3d-c0de-4af2-9490-eb7f0456336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_int2str(row):\n",
    "    return emotions['train'].features['label'].int2str(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f467f6ce-0ecb-438f-a0bf-abaadb6f5a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label label_name\n",
       "0                            i didnt feel humiliated      0    sadness\n",
       "1  i can go from feeling so hopeless to so damned...      0    sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong      3      anger\n",
       "3  i am ever feeling nostalgic about the fireplac...      2       love\n",
       "4                               i am feeling grouchy      3      anger"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label_name'] = df['label'].apply(label_int2str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164193d3-531f-414d-8013-f86cda721fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13b1ff1-4889-40aa-8572-bb6bb795d2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d183fd-2311-46d5-a91c-11f427599044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2e5a1d-823e-46df-84cf-c1e3044a621f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
