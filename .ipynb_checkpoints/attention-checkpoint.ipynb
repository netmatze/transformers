{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d08b6a29-344d-4064-bb85-f4d7e3a9ff85",
   "metadata": {},
   "source": [
    "## create a simple self attention mechanism to train sentiment analysis at the imdb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62d79988-462e-434a-84c7-678bd15a4506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text  I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV. It's not. It's clich√©d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it's rubbish as they have to always say \"Gene Roddenberry's Earth...\" otherwise people would not continue watching. Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.\n",
      "text  Worth the entertainment value of a rental, especially if you like action movies. This one features the usual car chases, fights with the great Van Damme kick style, shooting battles with the 40 shell load shotgun, and even terrorist style bombs. All of this is entertaining and competently handled but there is nothing that really blows you away if you've seen your share before.<br /><br />The plot is made interesting by the inclusion of a rabbit, which is clever but hardly profound. Many of the characters are heavily stereotyped -- the angry veterans, the terrified illegal aliens, the crooked cops, the indifferent feds, the bitchy tough lady station head, the crooked politician, the fat federale who looks like he was typecast as the Mexican in a Hollywood movie from the 1940s. All passably acted but again nothing special.<br /><br />I thought the main villains were pretty well done and fairly well acted. By the end of the movie you certainly knew who the good guys were and weren't. There was an emotional lift as the really bad ones got their just deserts. Very simplistic, but then you weren't expecting Hamlet, right? The only thing I found really annoying was the constant cuts to VDs daughter during the last fight scene.<br /><br />Not bad. Not good. Passable 4.\n",
      "text  its a totally average film with a few semi-alright action sequences that make the plot seem a little better and remind the viewer of the classic van dam films. parts of the plot don't make sense and seem to be added in to use up time. the end plot is that of a very basic type that doesn't leave the viewer guessing and any twists are obvious from the beginning. the end scene with the flask backs don't make sense as they are added in and seem to have little relevance to the history of van dam's character. not really worth watching again, bit disappointed in the end production, even though it is apparent it was shot on a low budget certain shots and sections in the film are of poor directed quality\n",
      "text  STAR RATING: ***** Saturday Night **** Friday Night *** Friday Morning ** Sunday Night * Monday Morning <br /><br />Former New Orleans homicide cop Jack Robideaux (Jean Claude Van Damme) is re-assigned to Columbus, a small but violent town in Mexico to help the police there with their efforts to stop a major heroin smuggling operation into their town. The culprits turn out to be ex-military, lead by former commander Benjamin Meyers (Stephen Lord, otherwise known as Jase from East Enders) who is using a special method he learned in Afghanistan to fight off his opponents. But Jack has a more personal reason for taking him down, that draws the two men into an explosive final showdown where only one will walk away alive.<br /><br />After Until Death, Van Damme appeared to be on a high, showing he could make the best straight to video films in the action market. While that was a far more drama oriented film, with The Shepherd he has returned to the high-kicking, no brainer action that first made him famous and has sadly produced his worst film since Derailed. It's nowhere near as bad as that film, but what I said still stands.<br /><br />A dull, predictable film, with very little in the way of any exciting action. What little there is mainly consists of some limp fight scenes, trying to look cool and trendy with some cheap slo-mo/sped up effects added to them that sadly instead make them look more desperate. Being a Mexican set film, director Isaac Florentine has tried to give the film a Robert Rodriguez/Desperado sort of feel, but this only adds to the desperation.<br /><br />VD gives a particularly uninspired performance and given he's never been a Robert De Niro sort of actor, that can't be good. As the villain, Lord shouldn't expect to leave the beeb anytime soon. He gets little dialogue at the beginning as he struggles to muster an American accent but gets mysteriously better towards the end. All the supporting cast are equally bland, and do nothing to raise the films spirits at all.<br /><br />This is one shepherd that's strayed right from the flock. *\n",
      "text  First off let me say, If you haven't enjoyed a Van Damme movie since bloodsport, you probably will not like this movie. Most of these movies may not have the best plots or best actors but I enjoy these kinds of movies for what they are. This movie is much better than any of the movies the other action guys (Segal and Dolph) have thought about putting out the past few years. Van Damme is good in the movie, the movie is only worth watching to Van Damme fans. It is not as good as Wake of Death (which i highly recommend to anyone of likes Van Damme) or In hell but, in my opinion it's worth watching. It has the same type of feel to it as Nowhere to Run. Good fun stuff!\n",
      "text  I had high hopes for this one until they changed the name to 'The Shepherd : Border Patrol, the lamest movie name ever, what was wrong with just 'The Shepherd'. This is a by the numbers action flick that tips its hat at many classic Van Damme films. There is a nice bit of action in a bar which reminded me of hard target and universal soldier but directed with no intensity or flair which is a shame. There is one great line about 'being p*ss drunk and carrying a rabbit' and some OK action scenes let down by the cheapness of it all. A lot of the times the dialogue doesn't match the characters mouth and the stunt men fall down dead a split second before even being shot. The end fight is one of the better Van Damme fights except the Director tries to go a bit too John Woo and fails also introducing flashbacks which no one really cares about just gets in the way of the action which is the whole point of a van Damme film.<br /><br />Not good, not bad, just average generic action.\n",
      "text  Isaac Florentine has made some of the best western Martial Arts action movies ever produced. In particular US Seals 2, Cold Harvest, Special Forces and Undisputed 2 are all action classics. You can tell Isaac has a real passion for the genre and his films are always eventful, creative and sharp affairs, with some of the best fight sequences an action fan could hope for. In particular he has found a muse with Scott Adkins, as talented an actor and action performer as you could hope for. This is borne out with Special Forces and Undisputed 2, but unfortunately The Shepherd just doesn't live up to their abilities.<br /><br />There is no doubt that JCVD looks better here fight-wise than he has done in years, especially in the fight he has (for pretty much no reason) in a prison cell, and in the final showdown with Scott, but look in his eyes. JCVD seems to be dead inside. There's nothing in his eyes at all. It's like he just doesn't care about anything throughout the whole film. And this is the leading man.<br /><br />There are other dodgy aspects to the film, script-wise and visually, but the main problem is that you are utterly unable to empathise with the hero of the film. A genuine shame as I know we all wanted this film to be as special as it genuinely could have been. There are some good bits, mostly the action scenes themselves. This film had a terrific director and action choreographer, and an awesome opponent for JCVD to face down. This could have been the one to bring the veteran action star back up to scratch in the balls-out action movie stakes.<br /><br />Sincerely a shame that this didn't happen.\n",
      "text  It actually pains me to say it, but this movie was horrible on every level. The blame does not lie entirely with Van Damme as you can see he tried his best, but let's face it, he's almost fifty, how much more can you ask of him? I find it so hard to believe that the same people who put together Undisputed 2; arguably the best (western) martial arts movie in years, created this. Everything from the plot, to the dialog, to the editing, to the overall acting was just horribly put together and in many cases outright boring and nonsensical. Scott Adkins who's fight scenes seemed more like a demo reel, was also terribly underused and not even the main villain which is such a shame because 1) He is more than capable of playing that role and 2) The actual main villain was not only not intimidating at all but also quite annoying. Again, not blaming Van Damme. I will always be a fan, but avoid this one.\n",
      "text  Technically I'am a Van Damme Fan, or I was. this movie is so bad that I hated myself for wasting those 90 minutes. Do not let the name Isaac Florentine (Undisputed II) fool you, I had big hopes for this one, depending on what I saw in (Undisputed II), man.. was I wrong ??! all action fans wanted a big comeback for the classic action hero, but i guess we wont be able to see that soon, as our hero keep coming with those (going -to-a-border - far-away-town-and -kill -the-bad-guys- than-comeback- home) movies I mean for God's sake, we are in 2008, and they insist on doing those disappointing movies on every level. Why ??!!! Do your self a favor, skip it.. seriously.\n",
      "text  Honestly awful film, bad editing, awful lighting, dire dialog and scrappy screenplay.<br /><br />The lighting at is so bad there's moments you can't even see what's going on, I even tried to playing with the contrast and brightness so I could see something but that didn't help.<br /><br />They must have found the script in a bin, the character development is just as awful and while you hardly expect much from a Jean-Claude Van Damme film this one manages to hit an all time low. You can't even laugh at the cheesy'ness.<br /><br />The directing and editing are also terrible, the whole film follows an extremely tired routine and fails at every turn as it bumbles through the plot that is so weak it's just unreal.<br /><br />There's not a lot else to say other than it's really bad and nothing like Jean-Claude Van Damme's earlier work which you could enjoy.<br /><br />Avoid like the plaque, frankly words fail me in condemning this \"film\".\n",
      "text  This flick is a waste of time.I expect from an action movie to have more than 2 explosions and some shooting.Van Damme's acting is awful. He never was much of an actor, but here it is worse.He was definitely better in his earlier movies. His screenplay part for the whole movie was probably not more than one page of stupid nonsense one liners.The whole dialog in the film is a disaster, same as the plot.The title \"The Shepherd\" makes no sense. Why didn't they just call it \"Border patrol\"? The fighting scenes could have been better, but either they weren't able to afford it, or the fighting choreographer was suffering from lack of ideas.This is a cheap low type of action cinema.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98962d90bad64f25a399aa5e9c2c562f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 30522\n",
      "type(x_train) <class 'torch.Tensor'>\n",
      "type(x_train[0]) <class 'torch.Tensor'>\n",
      "type(x_test) <class 'torch.Tensor'>\n",
      "type(x_test[0]) <class 'torch.Tensor'>\n",
      "y_train.shape  torch.Size([25000])\n",
      "y_test.shape  torch.Size([25000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mathias\\AppData\\Local\\Temp\\ipykernel_19524\\2397169574.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train)\n",
      "C:\\Users\\mathias\\AppData\\Local\\Temp\\ipykernel_19524\\2397169574.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = torch.tensor(y_test)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "max_length = 80\n",
    "\n",
    "# Load the IMDb dataset from Hugging Face\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Convert the dataset to PyTorch format\n",
    "imdb_dataset.set_format(\"torch\")\n",
    "\n",
    "# Get the training and test datasets\n",
    "train_dataset = imdb_dataset[\"train\"]\n",
    "test_dataset = imdb_dataset[\"test\"]\n",
    "\n",
    "#counter = 0\n",
    "#for item in test_dataset:\n",
    "    #print(\"item \", item)\n",
    "    #print(\"text \", item[\"text\"])\n",
    "    #counter += 1\n",
    "    #if counter > 10:\n",
    "        #break\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")  # Replace with your desired tokenizer\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    #return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, padding=True, max_length=maxlen)\n",
    "    #return tokenizer(examples[\"text\"], return_tensors=\"pt\", truncation=False, padding=True, max_length=maxlen)\n",
    "    return tokenizer(examples[\"text\"], padding='max_length', truncation=True, max_length=max_length) \n",
    "\n",
    "imdb_dataset = imdb_dataset.map(tokenize_function, batched=False)\n",
    "\n",
    "train_dataset = imdb_dataset[\"train\"]\n",
    "test_dataset = imdb_dataset[\"test\"]\n",
    "\n",
    "# Extract features (x) and labels (y) from the datasets\n",
    "x_train = train_dataset[\"input_ids\"]\n",
    "y_train = train_dataset[\"label\"]\n",
    "x_test = test_dataset[\"input_ids\"]\n",
    "y_test = test_dataset[\"label\"]\n",
    "\n",
    "# Convert labels to torch tensors\n",
    "y_train = torch.tensor(y_train)\n",
    "y_test = torch.tensor(y_test)\n",
    "\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "print('Vocabulary size:', vocab_size)\n",
    "\n",
    "print(\"type(x_train)\",type(x_train))\n",
    "print(\"type(x_train[0])\",type(x_train[0]))\n",
    "print(\"type(x_test)\",type(x_test))\n",
    "print(\"type(x_test[0])\",type(x_test[0]))\n",
    "print(\"y_train.shape \",y_train.shape)\n",
    "print(\"y_test.shape \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33e99eed-ca21-4c97-ba1b-38bab62562ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(x_train) <class 'torch.Tensor'>\n",
      "type(x_train[0]) <class 'torch.Tensor'>\n",
      "type(x_test) <class 'torch.Tensor'>\n",
      "type(x_test[0]) <class 'torch.Tensor'>\n",
      "y_train.shape  torch.Size([25000])\n",
      "y_test.shape  torch.Size([25000])\n"
     ]
    }
   ],
   "source": [
    "print(\"type(x_train)\",type(x_train))\n",
    "print(\"type(x_train[0])\",type(x_train[0]))\n",
    "print(\"type(x_test)\",type(x_test))\n",
    "print(\"type(x_test[0])\",type(x_test[0]))\n",
    "print(\"y_train.shape \",y_train.shape)\n",
    "print(\"y_test.shape \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e781b6c-7f50-4cfc-8c3e-3b5249460a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25000, 80])\n",
      "torch.Size([25000, 80])\n",
      "torch.Size([25000, 80])\n",
      "torch.Size([25000, 80])\n"
     ]
    }
   ],
   "source": [
    "x_train_tensor = x_train\n",
    "x_test_tensor = x_test\n",
    "y_train_tensor = y_train\n",
    "y_test_tensor = y_test  # Output: torch.Size([3, 3])\n",
    "\n",
    "# Print the shape of the resulting tensor\n",
    "print(x_train_tensor.shape)  # Output: torch.Size([3, 3])\n",
    "print(x_test_tensor.shape)  # Output: torch.Size([3, 3])\n",
    "print(x_test_tensor.shape)  # Output: torch.Size([3, 3])\n",
    "print(x_train_tensor.shape)  # Output: torch.Size([3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3971539e-b7c0-4b8f-b70f-94dc274a30a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "Vocabulary size: 30522\n"
     ]
    }
   ],
   "source": [
    "# Get the tokenizer used for the IMDb dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "print(tokenizer)  # Output: transformers.BertTokenizer\n",
    "\n",
    "test_sentence = \"This movie is amazing and I loved it.\"\n",
    "inputs = tokenizer(test_sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "test_sentence_tensor = inputs.input_ids\n",
    "\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "print('Vocabulary size:', vocab_size)\n",
    "\n",
    "#Text: \"This movie was absolutely amazing! The acting, the story, everything was perfect.\" Label: Positive\n",
    "#Text: \"The worst movie I've ever seen. The plot was nonsensical and the acting was terrible.\" Label: Negative\n",
    "#Text: \"I loved the cinematography in this film. It was visually stunning.\" Label: Positive\n",
    "#Text: \"I was so bored during this movie. It felt like it would never end.\" Label: Negative\n",
    "#Text: \"The special effects in this movie were incredible. I was blown away.\" Label: Positive\n",
    "#Text: \"I can't believe I wasted my time on this movie. It was a complete disaster.\" Label: Negative\n",
    "#Text: \"The performances in this film were outstanding. I was truly impressed.\" Label: Positive\n",
    "#Text: \"This movie was a total snooze fest. I couldn't even make it to the end.\" Label: Negative\n",
    "#Text: \"The soundtrack in this film was fantastic. It really added to the overall experience.\" Label: Positive\n",
    "#Text: \"The dialogue in this movie was so cringeworthy. I couldn't take it seriously.\" Label: Negative\n",
    "movie_sentiments = [\n",
    "    {\"text\": \"This movie was absolutely amazing! The acting, the story, everything was perfect.\", \"label\": \"Positive\"},\n",
    "    {\"text\": \"The worst movie I've ever seen. The plot was nonsensical and the acting was terrible.\", \"label\": \"Negative\"},\n",
    "    {\"text\": \"I loved the cinematography in this film. It was visually stunning.\", \"label\": \"Positive\"},\n",
    "    {\"text\": \"I was so bored during this movie. It felt like it would never end.\", \"label\": \"Negative\"},\n",
    "    {\"text\": \"The special effects in this movie were incredible. I was blown away.\", \"label\": \"Positive\"},\n",
    "    {\"text\": \"I can't believe I wasted my time on this movie. It was a complete disaster.\", \"label\": \"Negative\"},\n",
    "    {\"text\": \"The performances in this film were outstanding. I was truly impressed.\", \"label\": \"Positive\"},\n",
    "    {\"text\": \"This movie was a total snooze fest. I couldn't even make it to the end.\", \"label\": \"Negative\"},\n",
    "    {\"text\": \"The soundtrack in this film was fantastic. It really added to the overall experience.\", \"label\": \"Positive\"},\n",
    "    {\"text\": \"The dialogue in this movie was so cringeworthy. I couldn't take it seriously.\", \"label\": \"Negative\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "43b1081d-c611-4514-bc5f-cb99e77e10cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0366, -0.1959, -0.6939],\n",
      "        [ 0.0432, -0.1044,  0.2539]])\n",
      "tensor([[-1.0366,  0.0432],\n",
      "        [-0.1959, -0.1044],\n",
      "        [-0.6939,  0.2539]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "print(x)\n",
    "x_transposed = torch.transpose(x, 0, 1)\n",
    "print(x_transposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "692db6b1-739a-46e9-bb52-74cdefa07077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.w_query = nn.Linear(input_dim, input_dim)\n",
    "        self.w_key = nn.Linear(input_dim, input_dim)\n",
    "        self.w_value = nn.Linear(input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.w_query(x)\n",
    "        keys = self.w_key(x)\n",
    "        values = self.w_value(x)\n",
    "        keys_transposed = torch.transpose(keys, 0, 1)\n",
    "        keys_transposed = keys_transposed.permute(0, 2, 1)\n",
    "        #print(queries.shape)\n",
    "        #print(keys.shape)\n",
    "        #print(keys_transposed.shape)                \n",
    "        matmul_queries_keys = torch.matmul(queries, keys_transposed)\n",
    "        print(matmul_queries_keys.shape)\n",
    "        matmul_queries_keys_normalized = matmul_queries_keys / math.sqrt(self.input_dim)\n",
    "        attention = self.softmax(matmul_queries_keys_normalized)\n",
    "        print(matmul_queries_keys_normalized.shape)\n",
    "        print(values.shape)\n",
    "        print(attention.shape)\n",
    "        # Matrix multiplication using @ operator (PyTorch 1.10 and later)\n",
    "        result = attention @ values\n",
    "        print(result)  # Output: tensor([[19, 22], [43, 50]])\n",
    "        # weighted_attention = torch.matmul(attention, values)\n",
    "        return weighted_attention\n",
    "\n",
    "class SimpleSentimentClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_length):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.attention = SelfAttention(embed_dim)\n",
    "        #self.flatten = nn.Flatten()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(embed_dim * max_length, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #x = self.flatten(x)\n",
    "        #x = self.relu(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def get_embedding_shape(self):\n",
    "        return self.embedding.embedding_dim, self.embedding.weight.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2baf319e-b58e-41f5-b459-78557dd81890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n",
      "Embedding shape: (8, 30522)\n",
      "torch.Size([80, 80, 1])\n",
      "torch.Size([80, 80, 1])\n",
      "torch.Size([1, 80, 8])\n",
      "torch.Size([80, 80, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (6400x1 and 80x8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m input_sample \u001b[38;5;241m=\u001b[39m x_train_tensor[i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     32\u001b[0m target_sample \u001b[38;5;241m=\u001b[39m y_train_tensor[i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m---> 33\u001b[0m output \u001b[38;5;241m=\u001b[39m model(input_sample)\n\u001b[0;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39msqueeze(), target_sample\u001b[38;5;241m.\u001b[39msqueeze())        \n\u001b[0;32m     35\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mE:\\Programme\\anaconda\\envs\\custom_transformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Programme\\anaconda\\envs\\custom_transformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[63], line 46\u001b[0m, in \u001b[0;36mSimpleSentimentClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     43\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m#x = self.flatten(x)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m#x = self.relu(x)\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(x)\n\u001b[0;32m     47\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(x)\n\u001b[0;32m     48\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(x)\n",
      "File \u001b[1;32mE:\\Programme\\anaconda\\envs\\custom_transformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Programme\\anaconda\\envs\\custom_transformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[63], line 29\u001b[0m, in \u001b[0;36mSelfAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(values\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(attention\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 29\u001b[0m weighted_attention \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attention, values)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weighted_attention\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (6400x1 and 80x8)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Set the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device: \", device)\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = SimpleSentimentClassifier(vocab_size=vocab_size, embed_dim=8, max_length=max_length).to(device)\n",
    "embedding_shape = model.get_embedding_shape()\n",
    "print(\"Embedding shape:\", embedding_shape)\n",
    "criterion = nn.BCELoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert the data to the device (CPU or GPU)\n",
    "x_train_tensor = x_train_tensor.to(device)\n",
    "y_train_tensor = y_train_tensor.to(device)\n",
    "x_test_tensor = x_test_tensor.to(device)\n",
    "y_test_tensor = y_test_tensor.to(device)\n",
    "\n",
    "# Training loop without batches\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i in range(len(x_train_tensor)):\n",
    "        optimizer.zero_grad()\n",
    "        input_sample = x_train_tensor[i].unsqueeze(0)\n",
    "        target_sample = y_train_tensor[i].unsqueeze(0).float()\n",
    "        output = model(input_sample)\n",
    "        loss = criterion(output.squeeze(), target_sample.squeeze())        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        #print(\"training loss \", loss.item())\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_test_tensor)):\n",
    "            test_input_sample = x_test_tensor[i].unsqueeze(0)\n",
    "            test_target_sample = y_test_tensor[i].unsqueeze(0).float()\n",
    "            test_outputs = model(test_input_sample)\n",
    "            #print(\"test_outputs[0] \", test_outputs[0])\n",
    "            #print(\"y_test_tensor \", test_target_sample[0])\n",
    "            test_loss = criterion(test_outputs.squeeze(), test_target_sample.squeeze())\n",
    "            #print(\"test loss \", test_loss.item())                \n",
    "        test_outputs = model(x_test_tensor)\n",
    "        #print(\"test_outputs.shape \",test_outputs.shape)\n",
    "        #print(\"y_train_tensor.shape \",y_train_tensor.shape)\n",
    "        #print(\"test_outputs \",test_outputs[0])\n",
    "        #print(\"y_train_tensor \",y_train_tensor[0])\n",
    "        y_test_tensor = y_train_tensor.unsqueeze(1).float()       \n",
    "        #print(\"test_outputs[0] \", test_outputs[0])\n",
    "        #print(\"y_test_tensor \", y_test_tensor[0])\n",
    "        test_loss = criterion(test_outputs, y_test_tensor)\n",
    "        test_accuracy = ((test_outputs > 0.5).float() == y_test_tensor).sum().item() / len(y_test_tensor)\n",
    "    # print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(x_train_tensor)}\")\n",
    "    print(f\"Epoch: {epoch + 1}, Train Loss: {loss:.8f}, Test Loss: {test_loss:.8f}, Test Accuracy: {test_accuracy:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "062a32f1-9895-4692-bccf-55629fd40e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1045,  2293,  ..., 22580,  1010,   102],\n",
      "        [  101,  4276,  1996,  ...,  7987,  1013,   102],\n",
      "        [  101,  2049,  1037,  ...,  1996,  2927,   102],\n",
      "        ...,\n",
      "        [  101,  1045,  2288,  ...,  1998,  7134,   102],\n",
      "        [  101,  2274,  2781,  ...,  1012,  2059,   102],\n",
      "        [  101,  1045,  3236,  ...,  2000,  2377,   102]], device='cuda:0')\n",
      "test_outputs[0]  tensor([1.], device='cuda:0')\n",
      "y_test_tensor  tensor([0.], device='cuda:0')\n",
      "Test Loss: 45.253, Test Accuracy: 0.500\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(x_test_tensor)\n",
    "    y_test_tensor = y_train_tensor.unsqueeze(1).float() \n",
    "    print(x_test_tensor)\n",
    "    print(\"test_outputs[0] \", test_outputs[0])\n",
    "    print(\"y_test_tensor \", y_test_tensor[0])    \n",
    "    test_loss = criterion(test_outputs, y_test_tensor)\n",
    "    test_accuracy = ((test_outputs > 0.5).float() == y_test_tensor).sum().item() / len(y_test_tensor)\n",
    "print(f\"Test Loss: {test_loss:.3f}, Test Accuracy: {test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e57434a2-673f-430e-9eb9-e4b5d825b192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "Vocabulary size: 30522\n",
      "Sentiment for 'This movie was absolutely amazing! The acting, the story, everything was perfect.': Positive / Positive\n",
      "Sentiment for 'The worst movie I've ever seen. The plot was nonsensical and the acting was terrible.': Positive / Negative\n",
      "Sentiment for 'I loved the cinematography in this film. It was visually stunning.': Positive / Positive\n",
      "Sentiment for 'I was so bored during this movie. It felt like it would never end.': Positive / Negative\n",
      "Sentiment for 'The special effects in this movie were incredible. I was blown away.': Positive / Positive\n",
      "Sentiment for 'I can't believe I wasted my time on this movie. It was a complete disaster.': Positive / Negative\n",
      "Sentiment for 'The performances in this film were outstanding. I was truly impressed.': Positive / Positive\n",
      "Sentiment for 'This movie was a total snooze fest. I couldn't even make it to the end.': Positive / Negative\n",
      "Sentiment for 'The soundtrack in this film was fantastic. It really added to the overall experience.': Positive / Positive\n",
      "Sentiment for 'The dialogue in this movie was so cringeworthy. I couldn't take it seriously.': Positive / Negative\n",
      "Sentiment for 'The action scenes in this movie were breathtaking. I was on the edge of my seat the whole time.': Positive / Positive\n",
      "Sentiment for 'This movie was so predictable. I saw the ending coming from a mile away.': Positive / Negative\n",
      "Sentiment for 'The humor in this film was spot on. I was laughing the whole time.': Positive / Positive\n",
      "Sentiment for 'The pacing in this movie was terrible. It felt like it dragged on forever.': Positive / Negative\n",
      "Sentiment for 'The chemistry between the lead actors was incredible. I was completely invested in their story.': Positive / Positive\n",
      "Sentiment for 'The plot twists in this movie were ridiculous. I couldn't believe how unrealistic they were.': Positive / Negative\n",
      "Sentiment for 'The costumes in this film were amazing. They really added to the overall aesthetic.': Positive / Positive\n",
      "Sentiment for 'This movie was so confusing. I had no idea what was going on.': Positive / Negative\n",
      "Sentiment for 'The direction in this film was brilliant. The use of lighting and camera angles was very effective.': Positive / Positive\n",
      "Sentiment for 'The editing in this movie was awful. The transitions between scenes were jarring and disorienting.': Positive / Negative\n",
      "Accuracy 10.0\n"
     ]
    }
   ],
   "source": [
    "movie_sentiments = [\n",
    "    {\"text\": \"This movie was absolutely amazing! The acting, the story, everything was perfect.\", \"label\": \"Positive\"},\n",
    "    {\"text\": \"The worst movie I've ever seen. The plot was nonsensical and the acting was terrible.\", \"label\": \"Negative\"},\n",
    "    {\"text\": \"I loved the cinematography in this film. It was visually stunning.\", \"label\": \"Positive\"},\n",
    "    {\"text\": \"I was so bored during this movie. It felt like it would never end.\", \"label\": \"Negative\"},\n",
    "    {\"text\": \"The special effects in this movie were incredible. I was blown away.\", \"label\": \"Positive\"},\n",
    "    {\"text\": \"I can't believe I wasted my time on this movie. It was a complete disaster.\", \"label\": \"Negative\"},\n",
    "    {\"text\": \"The performances in this film were outstanding. I was truly impressed.\", \"label\": \"Positive\"},\n",
    "    {\"text\": \"This movie was a total snooze fest. I couldn't even make it to the end.\", \"label\": \"Negative\"},\n",
    "    {\"text\": \"The soundtrack in this film was fantastic. It really added to the overall experience.\", \"label\": \"Positive\"},\n",
    "    {\"text\": \"The dialogue in this movie was so cringeworthy. I couldn't take it seriously.\", \"label\": \"Negative\"},\n",
    "    {\"text\": \"The action scenes in this movie were breathtaking. I was on the edge of my seat the whole time.\", \"label\": \"Positive\"},\n",
    "    {\"text\": \"This movie was so predictable. I saw the ending coming from a mile away.\", \"label\": \"Negative\"},\n",
    "    {\"text\": \"The humor in this film was spot on. I was laughing the whole time.\", \"label\": \"Positive\"},\n",
    "    {\"text\": \"The pacing in this movie was terrible. It felt like it dragged on forever.\", \"label\": \"Negative\"},\n",
    "    {\"text\": \"The chemistry between the lead actors was incredible. I was completely invested in their story.\", \"label\": \"Positive\"},\n",
    "    {\"text\": \"The plot twists in this movie were ridiculous. I couldn't believe how unrealistic they were.\", \"label\": \"Negative\"},\n",
    "    {\"text\": \"The costumes in this film were amazing. They really added to the overall aesthetic.\", \"label\": \"Positive\"},\n",
    "    {\"text\": \"This movie was so confusing. I had no idea what was going on.\", \"label\": \"Negative\"},\n",
    "    {\"text\": \"The direction in this film was brilliant. The use of lighting and camera angles was very effective.\", \"label\": \"Positive\"},\n",
    "    {\"text\": \"The editing in this movie was awful. The transitions between scenes were jarring and disorienting.\", \"label\": \"Negative\"}\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "print(tokenizer)  # Output: transformers.BertTokenizer\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "print('Vocabulary size:', vocab_size)\n",
    "true_label = 0\n",
    "label_counter = 0\n",
    "for item in movie_sentiments:\n",
    "    test_sentence = item[\"text\"]\n",
    "    test_label = item[\"label\"]\n",
    "    \n",
    "    inputs = tokenizer(test_sentence, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=max_length)\n",
    "    test_sentence_tensor = inputs.input_ids\n",
    "    test_sentence_tensor = test_sentence_tensor.to(device)    \n",
    "    \n",
    "    prediction = model(test_sentence_tensor).item()\n",
    "    predictiontext = \"\"\n",
    "    if prediction > 0.5:\n",
    "        predictiontext = \"Positive\"\n",
    "    else:\n",
    "        predictiontext = \"Negative\"\n",
    "    if predictiontext == test_label:\n",
    "        true_label +=1\n",
    "    label_counter = 1    \n",
    "    print(f\"Sentiment for '{test_sentence}': {predictiontext} / {test_label}\")\n",
    "accuracy = true_label / label_counter\n",
    "print(f\"Accuracy {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f73ecd1c-4c34-49c9-b8fc-7d363997ce83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "Vocabulary size: 30522\n",
      "type(inputs)  <class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "type(test_sentence_tensor)  <class 'torch.Tensor'>\n",
      "Sentiment for 'This movie started great but then it got worse and worse.': Positive\n"
     ]
    }
   ],
   "source": [
    "# Get the tokenizer used for the IMDb dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "print(tokenizer)  # Output: transformers.BertTokenizer\n",
    "\n",
    "test_sentence = \"This movie is amazing and I loved it.\"\n",
    "test_sentence = \"This movie started great but then it got worse and worse.\"\n",
    "\n",
    "inputs = tokenizer(test_sentence, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=max_length)\n",
    "test_sentence_tensor = inputs.input_ids\n",
    "test_sentence_tensor = test_sentence_tensor.to(device)\n",
    "\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "print('Vocabulary size:', vocab_size)\n",
    "print(\"type(inputs) \", type(inputs))\n",
    "print(\"type(test_sentence_tensor) \", type(test_sentence_tensor))\n",
    "\n",
    "prediction = model(test_sentence_tensor).item()\n",
    "predictiontext = \"\"\n",
    "if prediction > 0.5:\n",
    "    predictiontext = \"Positive\"\n",
    "else:\n",
    "    predictiontext = \"Negative\"\n",
    "\n",
    "print(f\"Sentiment for '{test_sentence}': {predictiontext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb68aae-1ee7-47ea-a260-fac025c3923f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c548a82-ed05-4e62-af01-18944f71c7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1588d322-7a1a-496d-9ac8-505a3178ff9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9d995-ed4e-4847-afb4-bcf6a2e2e1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34ec9b0-18d4-48aa-b9a5-f7d19e90ca1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5580ec62-8470-4a29-b31a-c0e78252ddc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ee9aac4-203a-4a5e-874a-d219e9b27851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "# encoder representations of four different words\n",
    "word_1 = np.array([1, 0, 0])\n",
    "word_2 = np.array([0, 1, 0])\n",
    "word_3 = np.array([1, 1, 0])\n",
    "word_4 = np.array([0, 0, 1])\n",
    "print(word_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d98fcd4-97f3-4bf2-81c7-3fd60796263d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee2ad79-d42e-473a-bb14-8b485ab5bcd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa620d0-9ebf-48e8-807e-1e01151e3ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abf22708-282a-4304-b971-54edce400d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1]\n",
      " [0 0 1]\n",
      " [2 1 1]]\n",
      "[[2 0 0]\n",
      " [2 0 2]\n",
      " [2 2 0]]\n",
      "[[0 1 1]\n",
      " [1 2 1]\n",
      " [0 0 2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# generating the weight matrices\n",
    "random.seed(42) # to allow us to reproduce the same attention values\n",
    "W_Q = np.random.randint(3, size=(3, 3))\n",
    "W_K = np.random.randint(3, size=(3, 3))\n",
    "W_V = np.random.randint(3, size=(3, 3))\n",
    "print(W_Q)\n",
    "print(W_K)\n",
    "print(W_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b482933a-06ff-4f48-8841-40db2b3477bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0]\n",
      "*\n",
      "[[1 0 1]\n",
      " [0 0 1]\n",
      " [2 1 1]]\n",
      "=\n",
      "[1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# generating the queries, keys and values\n",
    "query_1 = word_1 @ W_Q\n",
    "key_1 = word_1 @ W_K\n",
    "value_1 = word_1 @ W_V\n",
    "\n",
    "query_2 = word_2 @ W_Q\n",
    "key_2 = word_2 @ W_K\n",
    "value_2 = word_2 @ W_V\n",
    "\n",
    "query_3 = word_3 @ W_Q\n",
    "key_3 = word_3 @ W_K\n",
    "value_3 = word_3 @ W_V\n",
    "\n",
    "query_4 = word_4 @ W_Q\n",
    "key_4 = word_4 @ W_K\n",
    "value_4 = word_4 @ W_V\n",
    "\n",
    "print(word_1)\n",
    "print('*')\n",
    "print(W_Q)\n",
    "print('=')\n",
    "print(query_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d13e8d68-f3c0-4349-921c-8188a3aeea3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 6 2]\n"
     ]
    }
   ],
   "source": [
    "# scoring the first query vector against all key vectors\n",
    "scores = np.array([np.dot(query_1, key_1), np.dot(query_1, key_2), np.dot(query_1, key_3), np.dot(query_1, key_4)])\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0103cd6-81c1-4bb6-9c29-d4889a609ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06561049 0.20818687 0.66059215 0.06561049]\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "# computing the weights by a softmax operation\n",
    "weights = softmax(scores / key_1.shape[0] ** 0.5)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "329c6603-cd5a-4b8a-9e33-1b7e364d489a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86877902 2.46376069 1.72620264]\n"
     ]
    }
   ],
   "source": [
    "# computing the attention by a weighted sum of the value vectors\n",
    "attention = (weights[0] * value_1) + (weights[1] * value_2) + (weights[2] * value_3) + (weights[3] * value_4)\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abe071a-d528-4a21-91d4-6b463d6434ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
