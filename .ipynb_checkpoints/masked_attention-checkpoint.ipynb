{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d78c32b-b838-489d-aefd-9b0714f34cd6",
   "metadata": {},
   "source": [
    "## Create template for Sentiment Analysis\n",
    "\n",
    "https://github.com/bentrevett/pytorch-sentiment-analysis/blob/main/1%20-%20Neural%20Bag%20of%20Words.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c2df9ef-4901-43e2-bc15-92dc7c47d2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1257769-39ee-494c-86ee-1f814b5dff34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 25000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 25000\n",
       " }))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data = datasets.load_dataset(\"imdb\", split=[\"train\", \"test\"])\n",
    "train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d4fb7b0-4f00-454b-8209-179a9ccb9c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f7bcc78243476ba53ae4b480addd3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838ac697189245b089995d568104efea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb532536c9ec43008701753c30241eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 30522\n",
      "type(x_train) <class 'torch.Tensor'>\n",
      "type(x_train[0]) <class 'torch.Tensor'>\n",
      "type(x_test) <class 'torch.Tensor'>\n",
      "type(x_test[0]) <class 'torch.Tensor'>\n",
      "y_train.shape  torch.Size([25000])\n",
      "y_test.shape  torch.Size([25000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mathias\\AppData\\Local\\Temp\\ipykernel_42996\\3074138392.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train)\n",
      "C:\\Users\\mathias\\AppData\\Local\\Temp\\ipykernel_42996\\3074138392.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = torch.tensor(y_test)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "max_length = 256\n",
    "\n",
    "# Load the IMDb dataset from Hugging Face\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Convert the dataset to PyTorch format\n",
    "imdb_dataset.set_format(\"torch\")\n",
    "\n",
    "# Get the training and test datasets\n",
    "train_dataset = imdb_dataset[\"train\"]\n",
    "test_dataset = imdb_dataset[\"test\"]\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")  # Replace with your desired tokenizer\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    #return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, padding=True, max_length=maxlen)\n",
    "    #return tokenizer(examples[\"text\"], return_tensors=\"pt\", truncation=False, padding=True, max_length=maxlen)\n",
    "    return tokenizer(examples[\"text\"], padding='max_length', truncation=True, max_length=max_length) \n",
    "\n",
    "imdb_dataset = imdb_dataset.map(tokenize_function, batched=False)\n",
    "\n",
    "train_dataset = imdb_dataset[\"train\"]\n",
    "test_dataset = imdb_dataset[\"test\"]\n",
    "\n",
    "# Extract features (x) and labels (y) from the datasets\n",
    "x_train = train_dataset[\"input_ids\"]\n",
    "y_train = train_dataset[\"label\"]\n",
    "x_test = test_dataset[\"input_ids\"]\n",
    "y_test = test_dataset[\"label\"]\n",
    "\n",
    "# Convert labels to torch tensors\n",
    "y_train = torch.tensor(y_train)\n",
    "y_test = torch.tensor(y_test)\n",
    "\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "print('Vocabulary size:', vocab_size)\n",
    "\n",
    "print(\"type(x_train)\",type(x_train))\n",
    "print(\"type(x_train[0])\",type(x_train[0]))\n",
    "print(\"type(x_test)\",type(x_test))\n",
    "print(\"type(x_test[0])\",type(x_test[0]))\n",
    "print(\"y_train.shape \",y_train.shape)\n",
    "print(\"y_test.shape \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1360404f-8268-4dd4-932c-1248f74d86db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25000, 256])\n",
      "torch.Size([25000, 256])\n",
      "torch.Size([25000])\n",
      "torch.Size([25000])\n"
     ]
    }
   ],
   "source": [
    "x_train_tensor = x_train\n",
    "x_test_tensor = x_test\n",
    "y_train_tensor = y_train\n",
    "y_test_tensor = y_test  # Output: torch.Size([3, 3])\n",
    "\n",
    "# Print the shape of the resulting tensor\n",
    "print(x_train_tensor.shape)  # Output: torch.Size([3, 3])\n",
    "print(x_test_tensor.shape)  # Output: torch.Size([3, 3])\n",
    "print(y_train_tensor.shape)  # Output: torch.Size([3, 3])\n",
    "print(y_test_tensor.shape)  # Output: torch.Size([3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3e3da52-5ffb-42aa-94e3-53da18b59b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 30522\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer\n",
    "import collections\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import tqdm\n",
    "\n",
    "max_length = 256\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Load the IMDb dataset from Hugging Face\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Convert the dataset to PyTorch format\n",
    "imdb_dataset.set_format(\"torch\")\n",
    "\n",
    "# Get the training and test datasets\n",
    "train_dataset = imdb_dataset[\"train\"]\n",
    "test_dataset = imdb_dataset[\"test\"]\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")  # Replace with your desired tokenizer\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    #return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, padding=True, max_length=maxlen)\n",
    "    #return tokenizer(examples[\"text\"], return_tensors=\"pt\", truncation=False, padding=True, max_length=maxlen)\n",
    "    return tokenizer(examples[\"text\"], padding='max_length', truncation=True, max_length=max_length) \n",
    "\n",
    "imdb_dataset = imdb_dataset.map(tokenize_function, batched=False)\n",
    "\n",
    "train_dataset = imdb_dataset[\"train\"]\n",
    "test_dataset = imdb_dataset[\"test\"]\n",
    "\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "print('Vocabulary size:', vocab_size)\n",
    "\n",
    "x_train = train_dataset[\"input_ids\"]\n",
    "y_train = train_dataset[\"label\"]\n",
    "x_test = test_dataset[\"input_ids\"]\n",
    "y_test = test_dataset[\"label\"]\n",
    "\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_data = TensorDataset(x_test, y_test)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a24f54f-223d-4e84-92d6-84cdfb8d40b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBoW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "\n",
    "    def forward(self, ids):\n",
    "        # ids = [batch size, seq len]\n",
    "        embedded = self.embedding(ids)\n",
    "        # embedded = [batch size, seq len, embedding dim]\n",
    "        pooled = embedded.mean(dim=1)\n",
    "        # pooled = [batch size, embedding dim]\n",
    "        prediction = self.fc(pooled)\n",
    "        # prediction = [batch size, output dim]\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f17b7e5-b519-44a6-849d-cb42b00d41bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "output_dim = 1\n",
    "\n",
    "model = NBoW(vocab_size, embedding_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49235ff3-e363-44c0-bcaa-7f7ebcad9422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 9,156,901 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f634fe3-793d-4c4d-bc7c-a80adbc315ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fbcb3a8-61e1-4b92-b714-e3f8ce7a0e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model#.to(device)\n",
    "criterion = criterion#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0efb17f-ecab-481d-8f4d-e33c07dabbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(prediction, label):\n",
    "    for item in prediction:\n",
    "        item_argmax = 0\n",
    "        if item < 0.0:\n",
    "            item_argmax = 0\n",
    "        else:\n",
    "            item_argmax = 1\n",
    "        print(item_argmax)\n",
    "    predicted_classes = prediction.argmax(dim=-1)\n",
    "    print(predicted_classes)\n",
    "    print(label)\n",
    "    correct_predictions = predicted_classes.eq(label).sum()\n",
    "    print(correct_predictions)\n",
    "    accuracy = correct_predictions / batch_size\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d2db5720-3ba2-47ee-b9dc-3fbd8cac507b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "tensor(18)\n",
      "tensor([0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "tensor(0)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#print(label.shape)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#print(prediction.dtype)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#print(label.dtype)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(prediction, label)\n\u001b[1;32m---> 21\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m get_accuracy(prediction, label)\n\u001b[0;32m     22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[46], line 15\u001b[0m, in \u001b[0;36mget_accuracy\u001b[1;34m(prediction, label)\u001b[0m\n\u001b[0;32m     13\u001b[0m correct_predictions \u001b[38;5;241m=\u001b[39m predicted_classes\u001b[38;5;241m.\u001b[39meq(label)\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(correct_predictions)\n\u001b[1;32m---> 15\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m correct_predictions \u001b[38;5;241m/\u001b[39m batch_size\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "metrics = collections.defaultdict(list)\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "    #for batch in tqdm.tqdm(train_dataloader, desc=\"training...\"):\n",
    "    #for step, batch in enumerate(train_dataloader):    \n",
    "    for batch_idx, (input_ids, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        ids = input_ids#.to(device)\n",
    "        label = labels#.to(device)\n",
    "        prediction = model(ids)#.to(device)\n",
    "        prediction = prediction.squeeze(-1)\n",
    "        #print(prediction.shape)\n",
    "        label = label.to(torch.float32)\n",
    "        #print(label.shape)\n",
    "        #print(prediction.dtype)\n",
    "        #print(label.dtype)\n",
    "        loss = criterion(prediction, label)\n",
    "        accuracy = get_accuracy(prediction, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        epoch_accs.append(accuracy.item())\n",
    "    epoch_losses = np.mean(epoch_losses)\n",
    "    epoch_accs = np.mean(epoch_accs)\n",
    "    metrics[\"train_losses\"].append(epoch_losses)\n",
    "    metrics[\"train_accs\"].append(epoch_accs)\n",
    "    print(f\"Epoch: {epoch + 1}, Train Loss: {epoch_losses:.8f}\")\n",
    "    #, Train Accuracy: {epoch_accs} Test Loss: {test_loss:.8f}, Test Accuracy: {test_accuracy:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e34779-9655-4960-89e2-0e439c2215f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404aaa64-96b0-4bb7-b121-893f657caf99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1795e78d-58f4-4cad-853d-a7905b3f60a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1439f67-2faf-4364-9dee-0d95d45c794e",
   "metadata": {},
   "source": [
    "## Decoder Architekture with masked attention for mlmatzeGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8464839c-4da0-4d64-9b2f-e5e9e9c3b307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
