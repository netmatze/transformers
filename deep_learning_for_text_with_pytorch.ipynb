{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56f02dd0-0f9a-4798-a8b7-514ada2d247b",
   "metadata": {},
   "source": [
    "## Deep Learning for text with PyTorch\n",
    "\n",
    "https://campus.datacamp.com/courses/deep-learning-for-text-with-pytorch/advanced-topics-in-deep-learning-for-text-with-pytorch?ex=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf2bc1e0-3c6f-4c4b-87e1-d27bab609e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "45c5e2c7-d868-4a85-a921-c5adec2def74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 3,  1, 10,  4,  3]), tensor([3, 1, 0, 6]), tensor([6, 1, 8, 1]), tensor([3, 1, 9, 6])]\n",
      "tensor([ 5,  7, 11,  2])\n"
     ]
    }
   ],
   "source": [
    "train_data = [\"the cat sat on the mat\",\"the cat wears a hat\",\"a cat eats cat food\",\"the cat paints a picture\"]\n",
    "\n",
    "#vocab = list(set(' '.join(data).split()))\n",
    "#vocab = {index: word for index, word in enumerate(vocab, start=1)}\n",
    "#print(vocab)\n",
    "\n",
    "vocab = set(' '.join(train_data).split())\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for word, i in word_to_ix.items()}\n",
    "pairs = [sentence.split() for sentence in train_data]\n",
    "#print(pairs)\n",
    "\n",
    "#input_data = []\n",
    "input_data_index = []\n",
    "#target_data = []\n",
    "target_data_index = []\n",
    "\n",
    "for pair in pairs:\n",
    "    paircounter = 1\n",
    "    #print(f\"pair: {pair}\")\n",
    "    #print(paircounter)\n",
    "    for word in pair:\n",
    "        words = pair[0:paircounter]\n",
    "        #print(f\"words: {words}\")       \n",
    "        input_data.append(words)\n",
    "        input_data_index.append(torch.tensor([word_to_ix[word] for word in words], dtype=torch.long))        \n",
    "        target_data.append([pair[paircounter]])\n",
    "        target_data_index.append(torch.tensor([word_to_ix[pair[paircounter]]], dtype=torch.long))\n",
    "        paircounter += 1    \n",
    "        if paircounter >= len(pair):\n",
    "            break        \n",
    "#print(input_data)  \n",
    "#print(input_data_index)  \n",
    "#print(target_data)  \n",
    "#print(target_data_index)  \n",
    "        \n",
    "input_data = [[word_to_ix[word] for word in sentence[:-1]] for sentence in pairs]\n",
    "target_data = [word_to_ix[sentence[-1]] for sentence in pairs]\n",
    "inputs = [torch.tensor(seq, dtype=torch.long) for seq in input_data]\n",
    "targets = torch.tensor(target_data, dtype=torch.long)\n",
    "vocab_size = len(vocab)\n",
    "#print(word_to_ix)\n",
    "print(inputs) \n",
    "print(targets)\n",
    "#print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e5f95d76-052c-4ba1-96f4-58aa9638638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 10\n",
    "hidden_dimension = 16\n",
    "\n",
    "class RNNWithAttentionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNWithAttentionModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dimension)\n",
    "        self.rnn = nn.RNN(embedding_dimension, hidden_dimension, batch_first=True)\n",
    "        self.attention = nn.Linear(hidden_dimension, 1)\n",
    "        self.fc = nn.Linear(hidden_dimension, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        attention_weights = torch.nn.functional.softmax(self.attention(out).squeeze(2), dim=1)\n",
    "        context = torch.sum(attention_weights.unsqueeze(2) * out, dim=1)\n",
    "        out = self.fc(context)\n",
    "        return out\n",
    "\n",
    "def pad_sequences(batch):\n",
    "    max_length = max([len(seq) for seq in batch])\n",
    "    return torch.stack([torch.cat([seq, torch.zeros(max_length - len(seq)).long()]) for seq in batch])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ebac5e11-4ee0-427d-a76d-af6c7800f9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 2.360353946685791\n",
      "epoch: 100, loss: 0.007466976065188646\n",
      "epoch: 200, loss: 0.0025251884944736958\n",
      "epoch: 300, loss: 0.0013571387389674783\n",
      "epoch: 400, loss: 0.000878841383382678\n",
      "epoch: 500, loss: 0.0006261293892748654\n",
      "epoch: 600, loss: 0.0004728807834908366\n",
      "epoch: 700, loss: 0.0003714765189215541\n",
      "epoch: 800, loss: 0.00030036226962693036\n",
      "epoch: 900, loss: 0.0002483417047187686\n",
      "epoch: 1000, loss: 0.00020901163225062191\n",
      "epoch: 1100, loss: 0.00017844037211034447\n",
      "epoch: 1200, loss: 0.00015418532711919397\n",
      "epoch: 1300, loss: 0.0001344590273220092\n",
      "epoch: 1400, loss: 0.00011830820585601032\n",
      "epoch: 1500, loss: 0.00010489866690477356\n",
      "epoch: 1600, loss: 9.351530752610415e-05\n",
      "epoch: 1700, loss: 8.377082122024149e-05\n",
      "epoch: 1800, loss: 7.551623275503516e-05\n",
      "epoch: 1900, loss: 6.827478500781581e-05\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "attention_model = RNNWithAttentionModel()\n",
    "optimizer = torch.optim.Adam(attention_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 2000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    attention_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    #for input in input_data_index:\n",
    "    #    print(input)    \n",
    "    padding_inputs = pad_sequences(inputs)\n",
    "    #print(f\"\\nInput: {' '.join([ix_to_word[int(ix)] for ix in inputs])}\")    \n",
    "    #print(f\"Target: {ix_to_word[int(target)]}\")    \n",
    "    #print(padding_inputs)\n",
    "    #print(target_data_index)\n",
    "    #print(padding_inputs)\n",
    "    outputs = attention_model(padding_inputs)        \n",
    "    #print(outputs)\n",
    "    #print(targets)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"epoch: {epoch}, loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "60bc55ba-a89f-4df6-ae80-260ced22597b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wears': 0, 'cat': 1, 'picture': 2, 'the': 3, 'on': 4, 'mat': 5, 'a': 6, 'hat': 7, 'eats': 8, 'paints': 9, 'sat': 10, 'food': 11}\n",
      "[[3, 1, 10, 4, 3], [3, 1, 0, 6], [6, 1, 8, 1], [3, 1, 9, 6]]\n",
      "[5, 7, 11, 2]\n",
      "\n",
      "Input: the cat sat on the\n",
      "Target: mat\n",
      "RNN with Attention prediction: mat\n",
      "tensor([[-2.0674, -2.0125, -1.9536, -2.2417, -2.8000, 10.3503, -2.0787, -3.5007,\n",
      "         -2.3193, -1.8649, -1.6632, -0.3335]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Input: the cat wears a\n",
      "Target: hat\n",
      "RNN with Attention prediction: hat\n",
      "tensor([[-2.1528, -2.0714,  0.8729, -2.5259, -2.7319, -0.1561, -3.5221,  7.7299,\n",
      "         -2.7464, -2.7942, -2.2278,  1.1865]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Input: a cat eats cat\n",
      "Target: food\n",
      "RNN with Attention prediction: food\n",
      "tensor([[-2.5573, -3.0988, -3.4908, -2.7104, -2.6637, -0.0671, -2.9777,  0.7935,\n",
      "         -3.4106, -2.0855, -2.8723, 11.0869]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Input: the cat paints a\n",
      "Target: picture\n",
      "RNN with Attention prediction: picture\n",
      "tensor([[-1.1960, -1.3891, 10.8273, -2.6054, -1.7074, -0.2757, -2.1791,  0.8863,\n",
      "         -1.8432, -1.7137, -1.0177, -4.3086]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(word_to_ix)\n",
    "print(input_data)\n",
    "print(target_data)\n",
    "#print(targets)\n",
    "#target_data = [0]\n",
    "#print(target_data)\n",
    "#print(type(target_data))\n",
    "\n",
    "for input_seq, target in zip(input_data, targets):\n",
    "    input_test = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0)\n",
    "    #print(input_test)\n",
    "    #print(target)\n",
    "    attention_model.eval()\n",
    "    attention_output = attention_model(input_test)\n",
    "    \n",
    "    attention_prediction = ix_to_word[torch.argmax(attention_output).item()]\n",
    "    \n",
    "    print(f\"\\nInput: {' '.join([ix_to_word[int(ix)] for ix in input_seq])}\")    \n",
    "    print(f\"Target: {ix_to_word[int(target)]}\")\n",
    "    print(f\"RNN with Attention prediction: {attention_prediction}\")\n",
    "    print(attention_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "f866a51a-26ff-43ab-9f19-cce3f149aa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1, 10, 4, 3]\n",
      "\n",
      "Input: the cat sat on the\n",
      "Target: mat\n",
      "RNN with Attention prediction: mat\n",
      "tensor([[-1.4444, -1.9390,  0.1107, -1.5883, -1.9620, 10.8326, -2.4085, -0.3996,\n",
      "         -1.5323, -1.9698, -1.6586, -2.7604]], grad_fn=<AddmmBackward0>)\n",
      "[3, 1, 0, 6]\n",
      "\n",
      "Input: the cat wears a\n",
      "Target: hat\n",
      "RNN with Attention prediction: hat\n",
      "tensor([[-2.4892, -2.6596, -0.3764, -2.4425, -2.5558,  0.5581, -2.1837, 10.5335,\n",
      "         -1.7379, -2.1922, -2.4021, -0.6459]], grad_fn=<AddmmBackward0>)\n",
      "[6, 1, 8, 1]\n",
      "\n",
      "Input: a cat eats cat\n",
      "Target: food\n",
      "RNN with Attention prediction: food\n",
      "tensor([[-1.7456, -2.2132, -0.0474, -1.8779, -2.0949, -4.2556, -2.5420, -0.3577,\n",
      "         -1.9348, -2.2835, -1.8511, 10.6666]], grad_fn=<AddmmBackward0>)\n",
      "[3, 1, 9, 6]\n",
      "\n",
      "Input: the cat paints a\n",
      "Target: picture\n",
      "RNN with Attention prediction: picture\n",
      "tensor([[-3.2474e+00, -3.5137e+00,  1.0952e+01, -2.5388e+00, -2.9565e+00,\n",
      "         -1.7396e-01, -3.3336e+00, -2.9747e-01, -2.4403e+00, -3.9164e+00,\n",
      "         -2.5756e+00, -1.0504e-02]], grad_fn=<AddmmBackward0>)\n",
      "[3]\n",
      "\n",
      "Input: the\n",
      "Target: cat\n",
      "RNN with Attention prediction: mat\n",
      "tensor([[-1.0375, -1.1628,  0.0825, -1.5732, -1.6391, 10.1650, -1.5815, -0.0598,\n",
      "         -1.3444, -1.6349, -1.3924, -4.6340]], grad_fn=<AddmmBackward0>)\n",
      "[3, 1]\n",
      "\n",
      "Input: the cat\n",
      "Target: wears\n",
      "RNN with Attention prediction: mat\n",
      "tensor([[-1.6586, -2.0054, -0.1440, -2.1009, -2.1783,  9.6391, -2.2015,  1.5744,\n",
      "         -1.6857, -2.0030, -1.7975, -2.7255]], grad_fn=<AddmmBackward0>)\n",
      "[1]\n",
      "\n",
      "Input: cat\n",
      "Target: eats\n",
      "RNN with Attention prediction: food\n",
      "tensor([[-2.2159, -2.7875, -0.5833, -2.1689, -2.6292, -0.7475, -2.9190,  0.8467,\n",
      "         -2.3855, -2.5448, -2.1186,  8.4506]], grad_fn=<AddmmBackward0>)\n",
      "[3, 1]\n",
      "\n",
      "Input: the cat\n",
      "Target: paints\n",
      "RNN with Attention prediction: mat\n",
      "tensor([[-1.6586, -2.0054, -0.1440, -2.1009, -2.1783,  9.6391, -2.2015,  1.5744,\n",
      "         -1.6857, -2.0030, -1.7975, -2.7255]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test_data = [\"the cat sat on the mat\",\"the cat wears a hat\",\"a cat eats cat food\",\"the cat paints a picture\",\n",
    "            \"the cat\", \"the cat wears\", \"cat eats\", \"the cat paints\"]\n",
    "test_pairs = [sentence.split() for sentence in test_data]\n",
    "test_input_data = [[word_to_ix[word] for word in sentence[:-1]] for sentence in test_pairs]\n",
    "test_target_data = [word_to_ix[sentence[-1]] for sentence in test_pairs]\n",
    "test_inputs = [torch.tensor(seq, dtype=torch.long) for seq in test_input_data]\n",
    "test_targets = torch.tensor(test_target_data, dtype=torch.long)\n",
    "\n",
    "for test_input_seq, test_target in zip(test_input_data, test_targets):\n",
    "    print(test_input_seq)\n",
    "    input_test = torch.tensor(test_input_seq, dtype=torch.long).unsqueeze(0)    \n",
    "    attention_model.eval()\n",
    "    attention_output = attention_model(input_test)\n",
    "    attention_prediction = ix_to_word[torch.argmax(attention_output).item()]\n",
    "    print(f\"\\nInput: {' '.join([ix_to_word[int(ix)] for ix in test_input_seq])}\")    \n",
    "    print(f\"Target: {ix_to_word[int(test_target)]}\")\n",
    "    print(f\"RNN with Attention prediction: {attention_prediction}\")\n",
    "    print(attention_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bd55e8-d71e-47c0-8eb1-5e6ddd7f8ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
